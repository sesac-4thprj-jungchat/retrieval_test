import os
import json
import torch
import traceback
from dotenv import load_dotenv

# LangChain ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.docstore.document import Document
from langchain.retrievers import ContextualCompressionRetriever, MultiQueryRetriever
from langchain_cohere import CohereRerank
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate

from llama_index.core.query_engine import RetrieverQueryEngine

from llama_index.core.retrievers import ReciprocalRankFusionRetriever

from llama_index.core.retrievers import RecursiveRetriever  # âœ… ìµœì‹  ë²„ì „ í˜¸í™˜


# LlamaIndex ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from llama_index.core import VectorStoreIndex, ServiceContext
from llama_index.core.query_engine import RetrieverQueryEngine

# Transformers ë° CrossEncoder ì„í¬íŠ¸
from transformers import AutoTokenizer, pipeline
from sentence_transformers import CrossEncoder

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()
if not os.getenv("HUGGINGFACE_HUB_TOKEN"):
    raise ValueError("HUGGINGFACE_HUB_TOKENì´ .env íŒŒì¼ì— ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
if not os.getenv("COHERE_API_KEY"):
    raise ValueError("COHERE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

# 1ï¸âƒ£ ë‚´ë¶€ LLM ì„¤ì • (Mistral-7B ì‚¬ìš©)
model_name = "mistralai/Mistral-7B-v0.2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
device = "cpu"
model = pipeline("text-generation", model=model_name, tokenizer=tokenizer, max_new_tokens=256, device=device)

# 2ï¸âƒ£ í—¬í¼ í•¨ìˆ˜
def clean_metadata(value):
    return value if value not in [None, ""] else "ì •ë³´ ì—†ìŒ"

def generate_text(obj):
    return f"ì„œë¹„ìŠ¤ëª…: {clean_metadata(obj.get('ì„œë¹„ìŠ¤ëª…'))}\nì„œë¹„ìŠ¤ID: {clean_metadata(obj.get('ì„œë¹„ìŠ¤ID'))}"

# 3ï¸âƒ£ JSON ë°ì´í„° ë¡œë“œ ë° Document ê°ì²´ ìƒì„±
json_file_path = "/Users/minjoo/Desktop/SeSac/final/20250304.json"  # ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½
try:
    with open(json_file_path, "r", encoding="utf-8") as f:
        json_data = json.load(f)
except FileNotFoundError:
    raise FileNotFoundError(f"JSON íŒŒì¼ '{json_file_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

documents = [Document(page_content=generate_text(obj), metadata={k: clean_metadata(obj.get(k)) for k in obj}) for obj in json_data]
print(f"ì´ {len(documents)}ê°œì˜ ë¬¸ì„œê°€ ë¡œë“œë¨.")

# 4ï¸âƒ£ FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±/ë¡œë“œ
embedding_model = HuggingFaceEmbeddings(model_name="dragonkue/snowflake-arctic-embed-l-v2.0-ko", model_kwargs={"device": "cuda"}, encode_kwargs={"normalize_embeddings": True})
persist_directory = "faiss_db"

try:
    vectorstore = FAISS.load_local(persist_directory, embedding_model, allow_dangerous_deserialization=True)
    print("ê¸°ì¡´ FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì„±ê³µ.")
except:
    vectorstore = FAISS.from_documents(documents, embedding_model)
    vectorstore.save_local(persist_directory)
    print(f"ì´ {len(documents)}ê°œì˜ ë¬¸ì„œê°€ FAISS ë²¡í„° DBì— ì €ì¥ë¨.")

# 5ï¸âƒ£ Cross-Encoder Re-rank ì„¤ì •
cross_encoder = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-12-v2", device=device)

def cross_encoder_rerank(query, documents, top_n=10):
    pairs = [[query, doc.page_content] for doc in documents]
    scores = cross_encoder.predict(pairs)
    sorted_docs = [doc for _, doc in sorted(zip(scores, documents), reverse=True)]
    return sorted_docs[:top_n]

# 6ï¸âƒ£ RAG Fusion + MultiQueryRetriever ì„¤ì •
index = VectorStoreIndex.from_vector_store(vectorstore)
service_context = ServiceContext.from_defaults(llm=model)

base_retriever = index.as_retriever(search_kwargs={"k": 30})
multi_query_prompt = PromptTemplate(input_variables=["question"], template="ì§ˆë¬¸: {question}\n3ê°œì˜ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±:\n1. [ì²« ë²ˆì§¸ ì¿¼ë¦¬]\n2. [ë‘ ë²ˆì§¸ ì¿¼ë¦¬]\n3. [ì„¸ ë²ˆì§¸ ì¿¼ë¦¬]")

multi_query_retriever = MultiQueryRetriever.from_llm(llm=model, retriever=base_retriever, prompt=multi_query_prompt)

# âœ… ìµœì‹  SimilarityFusionRetriever ì ìš© (RAG Fusion)
rag_fusion_retriever = RecursiveRetriever(base_retriever=multi_query_retriever, search_kwargs={"k": 15})

# 7ï¸âƒ£ Cohere Rerank ì ìš©
cohere_compressor = CohereRerank(model="rerank-multilingual-v3.0", top_n=10)
retriever_cohere_compressed = ContextualCompressionRetriever(base_compressor=cohere_compressor, base_retriever=rag_fusion_retriever)

# 8ï¸âƒ£ ìµœì¢… RAG Query Engine êµ¬ì„±
query_engine = RetrieverQueryEngine.from_args(retriever=retriever_cohere_compressed, service_context=service_context)

# 9ï¸âƒ£ LLM ì‘ë‹µ ìƒì„± ì²´ì¸
system_prompt = "ê²€ìƒ‰ëœ ë¬¸ì„œì—ì„œ, ì„œë¹„ìŠ¤ëª…ê³¼ ì„œë¹„ìŠ¤IDë¥¼ ë²ˆí˜¸ë³„ë¡œ ì¶œë ¥í•˜ì„¸ìš”.\nì¶œë ¥ í˜•ì‹: [ë²ˆí˜¸]. ì„œë¹„ìŠ¤ëª… (ì„œë¹„ìŠ¤ID)"
rag_prompt = ChatPromptTemplate.from_messages([("system", system_prompt), ("human", "{context}")])
combine_docs_chain = create_stuff_documents_chain(model, rag_prompt)
chain = create_retrieval_chain(retriever_cohere_compressed, combine_docs_chain)

print("ğŸ”¹ MultiQuery + RAG Fusion (Reciprocal Rank Fusion) + Re-rank (CrossEncoder + Cohere) ì„¤ì • ì™„ë£Œ!")

# ğŸ”Ÿ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰
if __name__ == "__main__":
    queries = [
        "ì„œìš¸ ë„ë´‰êµ¬ì— ì‚¬ëŠ” 34ì„¸ ì²­ë…„ ë‚¨ì„±ì—ê²Œ ë§ëŠ” ì„œë¹„ìŠ¤ë¥¼ ì°¾ì•„ì¤˜.",
        "ì „ë¼ë‚¨ë„ ëª©í¬ì‹œì— ì‚¬ëŠ” 48ì„¸ ì¤‘ë…„ ë‚¨ì„±ì—ê²Œ ë§ëŠ” ê·€ì–´ ê´€ë ¨ ì°½ì—… ì •ì±…ì„ ì•Œë ¤ì¤˜.",
        "ê°•ì›ë„ ë™í•´ì‹œì— ì‚¬ëŠ” 68ì„¸ ë…¸ì¸ ì—¬ì„±ì—ê²Œ ë§ëŠ” ì •ì±…ì„ ì°¾ì•„ì¤˜.",
    ]

    for i, query in enumerate(queries, 1):
        print(f"\n=== ì¿¼ë¦¬ {i}: {query} ===")
        try:
            response = query_engine.query(query)
            print("ğŸ”¹ ìµœì¢… ì‘ë‹µ:")
            print(response)
        except Exception as e:
            print(f"ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            traceback.print_exc()