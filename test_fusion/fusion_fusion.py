import os
import faiss
import json
import gc
import torch
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.retrievers import ContextualCompressionRetriever
from langchain_community.document_compressors.rankllm_rerank import RankLLMRerank
from langchain.chains import RetrievalQA
from langchain.docstore.document import Document
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.load import dumps, loads
from langchain_core.runnables import RunnablePassthrough
from operator import itemgetter

import torch
import gc
import multiprocessing

# # ë©€í‹°í”„ë¡œì„¸ì‹± ì•ˆì „í•˜ê²Œ ì„¤ì •
# if __name__ == '__main__':
#     multiprocessing.set_start_method("spawn", force=True)

# âœ… .env íŒŒì¼ ë¡œë“œ
load_dotenv("api.env")
if not os.getenv("HUGGINGFACE_HUB_TOKEN"):
    raise ValueError("âŒ HUGGINGFACE_HUB_TOKENì´ .env íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤.")

# âœ… FAISS ì¸ë±ìŠ¤ ê²½ë¡œ
persist_directory = "/Users/minjoo/Desktop/SeSac/final/"
index_file = os.path.join(persist_directory, "index.faiss")

# âœ… JSON ë°ì´í„° íŒŒì¼ ê²½ë¡œ
json_path = "/Users/minjoo/Desktop/SeSac/final/20250304.json"

# âœ… FAISS ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
use_existing_faiss = os.path.exists(index_file)

# âœ… ì„ë² ë”© ëª¨ë¸ ì„¤ì • (ê°€ë²¼ìš´ ëª¨ë¸ë¡œ ë³€ê²½)
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L12-v2",  # 1024ì°¨ì› ëª¨ë¸
    model_kwargs={"device": "cpu"},
    encode_kwargs={'normalize_embeddings': True}
)

# âœ… FAISS ì¸ë±ìŠ¤ ë¡œë“œ ë˜ëŠ” ìƒì„±
if use_existing_faiss:
    print(f"âœ… ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘: {index_file}")
    vectorstore = FAISS.load_local(persist_directory, embedding_model, allow_dangerous_deserialization=True)

    # âœ… FAISS ë©”ëª¨ë¦¬ ìºì‹œ ì¤„ì´ê¸°
    torch.set_grad_enabled(False)
    faiss.omp_set_num_threads(1)

    # âœ… FAISS ì¸ë±ìŠ¤ ìµœì í™”
    index = vectorstore.index
    print("âœ… FAISS ì¸ë±ìŠ¤ ìµœì í™” ì™„ë£Œ")

else:
    print("âš ï¸ ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ê°€ ì—†ì–´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")

    # âœ… ìƒˆë¡œìš´ FAISS ì¸ë±ìŠ¤ ìƒì„±
    index = faiss.IndexFlatL2(embedding_model.client.get_sentence_embedding_dimension())
    index = faiss.IndexIDMap(index)
    print("âœ… ìƒˆë¡œìš´ FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")

# âœ… Retriever ì„¤ì •
print("âœ… Retriever ì„¤ì • ì‹œì‘")
retriever = vectorstore.as_retriever(search_kwargs={"k": 20})
print("âœ… Retriever ì„¤ì • ì™„ë£Œ")

# âœ… RankLLM ì„¤ì • (RankGPT)
compressor = RankLLMRerank(
    top_n=2,
    model="gpt",
    gpt_model="gpt-3.5-turbo"  # ë” ì‘ì€ ëª¨ë¸ë¡œ êµì²´
)

compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor, base_retriever=retriever
)
print("âœ… RankLLM ì„¤ì • ì™„ë£Œ")

# âœ… RAG-Fusion ì¿¼ë¦¬ ìƒì„±
print("âœ… RAG-Fusion ì¿¼ë¦¬ ìƒì„± ì‹œì‘")
template = """Generate multiple search queries related to: {question} \n
Output (4 queries):"""
prompt_rag_fusion = ChatPromptTemplate.from_template(template)

# generate_queries = (
#     prompt_rag_fusion 
#     | ChatOpenAI(temperature=0, max_tokens=512, streaming=True)
#     | StrOutputParser() 
#     | (lambda x: x.split("\n"))
# )
generate_queries = (
    prompt_rag_fusion 
    | ChatOpenAI(temperature=0, max_tokens=512, streaming=True)
    | StrOutputParser()
    | (lambda x: x.split("\n") if isinstance(x, str) else x)  # ì—¬ëŸ¬ ê°œì˜ ì¿¼ë¦¬ê°€ ìƒì„±ë  ê°€ëŠ¥ì„± ìˆìŒ
    | (lambda x: x[0] if isinstance(x, list) and len(x) > 0 else x)  # ğŸ”¹ ë¦¬ìŠ¤íŠ¸ê°€ ë˜ë©´ ì²« ë²ˆì§¸ ê°’ë§Œ ì‚¬ìš©
)

print("âœ… RAG-Fusion ì¿¼ë¦¬ ìƒì„± ì™„ë£Œ")

# âœ… RAG-Fusion Reciprocal Rank Fusion
def reciprocal_rank_fusion(results: list[list], k=60):
    print("âœ… RAG-Fusion: ì‹œì‘")
    fused_scores = {}
    for docs in results:
        for rank, doc in enumerate(docs):
            doc_str = dumps(doc)
            if doc_str not in fused_scores:
                fused_scores[doc_str] = 0
            fused_scores[doc_str] += 1 / (rank + k)

    return [
        (loads(doc), score)
        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)
    ]


# âœ… RAG-Fusion Retrieval Chain (ë³‘ë ¬ ê²€ìƒ‰ ì œê±°)
retrieval_chain_rag_fusion = (
    generate_queries
    | retriever  # ğŸ”¹ retriever.map() ëŒ€ì‹  ì¼ë°˜ retriever ì‚¬ìš©
    | reciprocal_rank_fusion
)


# âœ… ìµœì¢… RAG-Fusion Query ì‹¤í–‰
query = "ê°•ì›ë„ ë™í•´ì‹œì— ì‚¬ëŠ” 68ì„¸ ë…¸ì¸ ì—¬ìì—ê²Œ ë§ëŠ” ì •ì±…ì„ ì°¾ì•„ì¤˜."
print(f"ğŸ” ì§ˆì˜ ìˆ˜í–‰ ì¤‘: {query}, íƒ€ì…: {type(query)}") 

docs = retrieval_chain_rag_fusion.invoke({"question": query})
print(f"âœ… {len(docs)}ê°œì˜ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ!")

# âœ… LLM ì‘ë‹µ ìƒì„±
template_rag = """Answer the following question based on this context:

{context}

Question: {question}
"""

prompt_rag = ChatPromptTemplate.from_template(template_rag)

llm = ChatOpenAI(temperature=0)

final_rag_chain = (
    {"context": docs, "question": query}  # ì§ì ‘ docsë¥¼ ì‚¬ìš©
    | prompt_rag
    | llm
    | StrOutputParser()
)

final_response = final_rag_chain.invoke({"docs": docs, "question": query})

print("\nğŸ”¹ ìµœì¢… ì‘ë‹µ:\n")
print(final_response)

# âœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬
gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
print("âœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ!")