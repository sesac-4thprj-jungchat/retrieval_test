import os
import json
import re
import torch
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.docstore.document import Document
from transformers import AutoTokenizer, AutoModelForCausalLM
from langsmith import Client  # LangSmith Client ì„í¬íŠ¸

import multiprocessing

import torch

if __name__ == "__main__":
    multiprocessing.set_start_method("spawn", force=True)

# .env íŒŒì¼ ë¡œë“œ (HUGGINGFACE_HUB_TOKEN, LANGSMITH_API_KEY ë“±ì´ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•¨)
load_dotenv('api.env')
if not os.getenv("HUGGINGFACE_HUB_TOKEN"):
    raise ValueError("HUGGINGFACE_HUB_TOKENì´ .env íŒŒì¼ì— ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
if not os.getenv("LANGSMITH_API_KEY"):
    raise ValueError("LANGSMITH_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

# LangSmith Client ì´ˆê¸°í™” ë° rag-fusion-query-generation prompt ë¶ˆëŸ¬ì˜¤ê¸°
LANGSMITH_API_KEY = os.getenv("LANGSMITH_API_KEY")
client = Client(api_key=LANGSMITH_API_KEY)
rag_fusion_prompt = client.pull_prompt("langchain-ai/rag-fusion-query-generation", include_model=True)

# ëª¨ë¸ ì´ë¦„ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
MODEL_NAME = "beomi/KoAlpaca-Polyglot-5.8B"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

device = "cpu"

# CPU ë˜ëŠ” MPS í™˜ê²½ì—ì„œëŠ” torch.float16 ëŒ€ì‹  torch.float32 ì‚¬ìš© ê¶Œì¥
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float32)
model.to(device)
tokenizer.pad_token = tokenizer.eos_token

# None ë˜ëŠ” ë¹ˆ ë¬¸ìì—´ì„ "ì •ë³´ ì—†ìŒ"ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def clean_metadata(value):
    return value if value not in [None, ""] else "ì •ë³´ ì—†ìŒ"

# JSON ë°ì´í„° ë¡œë“œ ë° Document ê°ì²´ ìƒì„±
json_file_path = r"./20250304.json"
try:
    with open(json_file_path, "r", encoding="utf-8") as f:
        json_data = json.load(f)
except FileNotFoundError:
    raise FileNotFoundError(f"JSON íŒŒì¼ '{json_file_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
except json.JSONDecodeError:
    raise ValueError(f"JSON íŒŒì¼ '{json_file_path}'ì˜ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")

if not isinstance(json_data, list):
    raise ValueError("JSON ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœê°€ ì•„ë‹™ë‹ˆë‹¤.")

documents = []
for obj in json_data:
    text = (
        f"ì„œë¹„ìŠ¤ëª…: {clean_metadata(obj.get('ì„œë¹„ìŠ¤ëª…'))}\n"
        f"ì„œë¹„ìŠ¤ID: {clean_metadata(obj.get('ì„œë¹„ìŠ¤ID'))}\n"
        f"ë¶€ì„œëª…: {clean_metadata(obj.get('ë¶€ì„œëª…'))}\n"
        f"ì„œë¹„ìŠ¤ë¶„ì•¼: {clean_metadata(obj.get('ì„œë¹„ìŠ¤ë¶„ì•¼'))}\n"
        f"ì„œë¹„ìŠ¤ëª©ì ìš”ì•½: {clean_metadata(obj.get('ì„œë¹„ìŠ¤ëª©ì ìš”ì•½'))}\n"
        f"ì§€ì›ëŒ€ìƒ: {clean_metadata(obj.get('ì§€ì›ëŒ€ìƒ'))}\n"
        f"ì§€ì›ë‚´ìš©: {clean_metadata(obj.get('ì§€ì›ë‚´ìš©'))}\n"
        f"ì„ ì •ê¸°ì¤€: {clean_metadata(obj.get('ì„ ì •ê¸°ì¤€'))}\n"
        f"ì§€ì›ìœ í˜•: {clean_metadata(obj.get('ì§€ì›ìœ í˜•'))}\n"
        f"ì‹ ì²­ê¸°í•œ: {clean_metadata(obj.get('ì‹ ì²­ê¸°í•œ'))}\n"
        f"ì‹ ì²­ë°©ë²•: {clean_metadata(obj.get('ì‹ ì²­ë°©ë²•'))}\n"
        f"ì ‘ìˆ˜ê¸°ê´€: {clean_metadata(obj.get('ì ‘ìˆ˜ê¸°ê´€'))}\n"
    )
    metadata = {
        "ì„œë¹„ìŠ¤ID": clean_metadata(obj.get("ì„œë¹„ìŠ¤ID")),
        "ì„œë¹„ìŠ¤ëª…": clean_metadata(obj.get("ì„œë¹„ìŠ¤ëª…")),
        "ì„œë¹„ìŠ¤ëª©ì ìš”ì•½": clean_metadata(obj.get("ì„œë¹„ìŠ¤ëª©ì ìš”ì•½")),
        "ì‹ ì²­ê¸°í•œ": clean_metadata(obj.get("ì‹ ì²­ê¸°í•œ")),
        "ì§€ì›ë‚´ìš©": clean_metadata(obj.get("ì§€ì›ë‚´ìš©")),
        "ì„œë¹„ìŠ¤ë¶„ì•¼": clean_metadata(obj.get("ì„œë¹„ìŠ¤ë¶„ì•¼")),
        "ì„ ì •ê¸°ì¤€": clean_metadata(obj.get("ì„ ì •ê¸°ì¤€")),
        "ì‹ ì²­ë°©ë²•": clean_metadata(obj.get("ì‹ ì²­ë°©ë²•")),
        "ë¶€ì„œëª…": clean_metadata(obj.get("ë¶€ì„œëª…")),
        "ì ‘ìˆ˜ê¸°ê´€": clean_metadata(obj.get("ì ‘ìˆ˜ê¸°ê´€"))
    }
    documents.append(Document(page_content=text, metadata=metadata))

# ì„ë² ë”© ëª¨ë¸ ì„¤ì • ë° FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±/ë¡œë“œ
embedding_model = HuggingFaceEmbeddings(
    model_name="dragonkue/snowflake-arctic-embed-l-v2.0-ko",
    model_kwargs={'device': device},  # deviceë¥¼ Mac í™˜ê²½ì— ë§ê²Œ ì§€ì •
    encode_kwargs={'normalize_embeddings': True}
)

persist_directory = "/Users/minjoo/Desktop/SeSac/final"
faiss_index_file = os.path.join(persist_directory, "index.faiss")  # index.faiss íŒŒì¼ ê²½ë¡œ ì§€ì •

if not os.path.exists(faiss_index_file):
    raise FileNotFoundError(f"FAISS ì¸ë±ìŠ¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {faiss_index_file}")

vectorstore = FAISS.load_local(persist_directory, embedding_model, allow_dangerous_deserialization=True)

# persist_directory = r"/Users/minjoo/Desktop/SeSac/final/index.faiss"
# try:
#     vectorstore = FAISS.load_local(persist_directory, embedding_model, allow_dangerous_deserialization=True)
# except Exception as e:
#     raise Exception(f"FAISS ë²¡í„° DB ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# âœ… ì¼ë°˜ RAG ê²€ìƒ‰: ì¿¼ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
def retrieve_documents(query, top_k=20):
    return vectorstore.similarity_search(query, k=top_k)

# âœ… RAG Fusion: ì¿¼ë¦¬ ë²ˆì—­ ë‹¨ê³„ì— ì‚¬ìš© (LangSmithì—ì„œ ê°€ì ¸ì˜¨ prompt í™œìš©)
def translate_query_with_rag_fusion(query):
    input_text = f"{rag_fusion_prompt}\nì‚¬ìš©ì ì¿¼ë¦¬: {query}\në²ˆì—­ëœ ì¿¼ë¦¬:"

    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=512, padding=True).to("cpu")

    print("ğŸš€ Generating translation...")  # ğŸ”¹ ë””ë²„ê¹…ìš© ì¶œë ¥ ì¶”ê°€

    outputs = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=10,  # ğŸ”¹ í† í° ê°œìˆ˜ ì¤„ì´ê¸° â†’ ì†ë„ ê°œì„ 
        do_sample=False,
        no_repeat_ngram_size=3
    )

    translated_query = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print("âœ… Translation complete!")  # ğŸ”¹ ë””ë²„ê¹…ìš© ì¶œë ¥ ì¶”ê°€

    return translated_query


# ğŸ”¹ ëª¨ë¸ ì†ë„ ìµœì í™” ì ìš©
model = torch.compile(model)


# âœ… GPT Rank (KoGPT-6B-4bit ì‚¬ìš©)
def rank_documents_with_kogpt(query, documents):
    prompt = "ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ê°€ì¥ ê´€ë ¨ ìˆëŠ” ë¬¸ì„œë¥¼ ìˆœì„œëŒ€ë¡œ ë‚˜ì—´í•˜ê³  ê°ê°ì˜ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ 1ì ì—ì„œ 100ì  ì‚¬ì´ë¡œ ë§¤ê²¨ì£¼ì„¸ìš”.\n\n"
    for i, doc in enumerate(documents):
        prompt += f"{i+1}. {doc.page_content}\n"
    prompt += "\nê° ë¬¸ì„œì˜ ìˆœìœ„ì™€ ì ìˆ˜ë¥¼ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”: (ìˆœìœ„). (ë¬¸ì„œ ë‚´ìš© ì¼ë¶€) - (ì ìˆ˜)ì "

    kogpt_model_name = "rycont/kakaobrain__kogpt-6b-4bit"
    kogpt_tokenizer = AutoTokenizer.from_pretrained(kogpt_model_name)
    kogpt_model = AutoModelForCausalLM.from_pretrained(kogpt_model_name, torch_dtype=torch.float16).to(device)

    inputs = kogpt_tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024).to(device)
    outputs = kogpt_model.generate(inputs["input_ids"], max_new_tokens=200)
    ranked_text = kogpt_tokenizer.decode(outputs[0], skip_special_tokens=True)

    def extract_rank_score_kogpt(ranked_text, document):
        doc_snippet = document.page_content[:30]
        pattern = re.compile(rf"{re.escape(doc_snippet)}.*?(\d+)ì ", re.DOTALL)
        match = pattern.search(ranked_text)
        return int(match.group(1)) if match else 0

    ranked_docs = sorted(documents, key=lambda x: extract_rank_score_kogpt(ranked_text, x), reverse=True)
    return ranked_docs[:15]

# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
query = "ì„œìš¸íŠ¹ë³„ì‹œ ë„ë´‰êµ¬ì— ì‚¬ëŠ” 34ì„¸ ì²­ë…„ ë‚¨ìì—ê²Œ ë§ëŠ” ì„œë¹„ìŠ¤ë¥¼ ì°¾ì•„ì¤˜."

print("ğŸ” 1ë‹¨ê³„: RAG Fusionì„ ì´ìš©í•œ ì¿¼ë¦¬ ë²ˆì—­")
translated_query = translate_query_with_rag_fusion(query)
print("ë²ˆì—­ëœ ì¿¼ë¦¬:", translated_query)

print("ğŸ” 2ë‹¨ê³„: ì¼ë°˜ RAG ê²€ìƒ‰")
retrieved_docs = retrieve_documents(translated_query)

print("ğŸ“Š 3ë‹¨ê³„: GPT Rankë¥¼ ì´ìš©í•œ ë¬¸ì„œ ë­í‚¹ (KoGPT-6B-4bit)")
ranked_docs = rank_documents_with_kogpt(translated_query, retrieved_docs)

print("âœ… ìµœì¢… ë­í¬ ê²°ê³¼:")
for i, doc in enumerate(ranked_docs):
    print(f"{i+1}. {doc.metadata.get('ì„œë¹„ìŠ¤ëª…', 'ì„œë¹„ìŠ¤ëª… ì—†ìŒ')}")


# ë¦¬ì†ŒìŠ¤ ë¹„ì›€
import gc

if __name__ == "__main__":
    print("ğŸ”„ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
    torch.mps.empty_cache()  # MPS ë©”ëª¨ë¦¬ í•´ì œ
    for p in multiprocessing.active_children():
        p.terminate()  # ì—´ë¦° í”„ë¡œì„¸ìŠ¤ ê°•ì œ ì¢…ë£Œ
    gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
    print("âœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ!")
