import os
import faiss
import json
import gc
import torch
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.retrievers.contextual_compression import ContextualCompressionRetriever
from langchain_community.document_compressors.rankllm_rerank import RankLLMRerank
from langchain.chains import RetrievalQA
from langchain.docstore.document import Document

# âœ… .env íŒŒì¼ ë¡œë“œ
load_dotenv("api.env")
if not os.getenv("HUGGINGFACE_HUB_TOKEN"):
    raise ValueError("âŒ HUGGINGFACE_HUB_TOKENì´ .env íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤.")

# âœ… FAISS ì¸ë±ìŠ¤ ê²½ë¡œ
persist_directory = "/Users/minjoo/Desktop/SeSac/final/"
index_file = os.path.join(persist_directory, "index.faiss")

# âœ… JSON ë°ì´í„° íŒŒì¼ ê²½ë¡œ
json_path = "/Users/minjoo/Desktop/SeSac/final/20250304.json"

# âœ… FAISS ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
use_existing_faiss = os.path.exists(index_file)

if use_existing_faiss:
    print(f"âœ… ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘: {index_file}")
else:
    print("âš ï¸ FAISS ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")

# âœ… HuggingFace ì„ë² ë”© ëª¨ë¸ ì„¤ì •
embedding_model = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    model_kwargs={'device': 'cpu'},  # 'torch_dtype' ì œê±°
    encode_kwargs={'normalize_embeddings': True}
)

# âœ… FAISS ì¸ë±ìŠ¤ ë¡œë“œ ë˜ëŠ” ìƒì„±
if use_existing_faiss:
    vectorstore = FAISS.load_local(persist_directory, embedding_model, allow_dangerous_deserialization=True)
else:
    # âœ… JSON íŒŒì¼ ë¡œë“œ
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"âŒ JSON ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
    
    with open(json_path, "r", encoding="utf-8") as f:
        json_docs = json.load(f)
    
    print(f"ğŸ” JSON ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: ì´ {len(json_docs)}ê°œ ë¬¸ì„œ")

    # âœ… ë¬¸ì„œ ë³€í™˜ ë° FAISS ìƒì„±
    documents = []
    for obj in json_docs:
        try:
            # âœ… í•„ìˆ˜ í‚¤ ê²€ì¦ ë° ê¸°ë³¸ê°’ ì²˜ë¦¬
            text = (
                f"ì„œë¹„ìŠ¤ëª…: {obj.get('ì„œë¹„ìŠ¤ëª…', 'ì •ë³´ ì—†ìŒ')}\n"
                f"ì„œë¹„ìŠ¤ID: {obj.get('ì„œë¹„ìŠ¤ID', 'ì •ë³´ ì—†ìŒ')}\n"
                f"ì„œë¹„ìŠ¤ëª©ì ìš”ì•½: {obj.get('ì„œë¹„ìŠ¤ëª©ì ìš”ì•½', 'ì •ë³´ ì—†ìŒ')}\n"
                f"ì§€ì›ë‚´ìš©: {obj.get('ì§€ì›ë‚´ìš©', 'ì •ë³´ ì—†ìŒ')}\n"
            )
            documents.append(Document(page_content=text))  # metadata ìµœì†Œí™”í•˜ì—¬ FAISS ì €ì¥ ë¬¸ì œ ë°©ì§€
        
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e} â†’ í•´ë‹¹ ê°ì²´: {obj}")

    # âœ… í…ìŠ¤íŠ¸ ë¶„í•  í›„ FAISS ì¸ë±ìŠ¤ ìƒì„±
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    texts = text_splitter.split_documents(documents)

    vectorstore = FAISS.from_documents(texts, embedding_model)
    vectorstore.save_local(persist_directory)
    print("âœ… ìƒˆë¡œìš´ FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ!")

# âœ… Retriever ì„¤ì •
retriever = vectorstore.as_retriever(search_kwargs={"k": 20})

# âœ… RankLLM ì„¤ì • (RankGPT)
compressor = RankLLMRerank(
    top_n=2,  
    model="gpt",  
    gpt_model="gpt-4o"  
)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor, base_retriever=retriever
)

print("âœ… RankLLM ì„¤ì • ì™„ë£Œ")

# âœ… LLM (GPT-4o) ì„¤ì •
llm = ChatOpenAI(temperature=0)
chain = RetrievalQA.from_chain_type(llm=llm, retriever=compression_retriever)

# âœ… ì§ˆì˜ ì‹¤í–‰
query = "ê°•ì›ë„ ë™í•´ì‹œì— ì‚¬ëŠ” 68ì„¸ ë…¸ì¸ ì—¬ìì—ê²Œ ë§ëŠ” ì •ì±…ì„ ì°¾ì•„ì¤˜. "
print(f"ğŸ” ì§ˆì˜ ìˆ˜í–‰ ì¤‘: {query}")

retrieved_docs = retriever.invoke(query)
print(f"âœ… {len(retrieved_docs)}ê°œì˜ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ!")

# âœ… RAG-Fusion ìµœì¢… ì¶œë ¥ í˜•ì‹ ë³€í™˜
def rag_fusion_final(query, docs):
    top_docs = docs[:15]
    docs_text = "\n\n".join(
        f"ì„œë¹„ìŠ¤ID: {doc.metadata.get('ì„œë¹„ìŠ¤ID', 'N/A')}\n"
        f"ì„œë¹„ìŠ¤ëª…: {doc.metadata.get('ì„œë¹„ìŠ¤ëª…', 'N/A')}\n"
        f"{((doc.metadata.get('ì„œë¹„ìŠ¤ëª©ì ìš”ì•½') or '') + ' ' + (doc.metadata.get('ì§€ì›ë‚´ìš©') or '')).strip()}"
        for doc in top_docs
    )
    final_prompt = f"Query: {query}\n\nDocuments:\n{docs_text}\n\nAnswer:"
    return final_prompt

# âœ… ìµœì¢… ì‹¤í–‰ ê²°ê³¼
final_output = rag_fusion_final(query, retrieved_docs)
print("\nğŸ”¹ RAG-Fusion ìµœì¢… ì¶œë ¥:\n")
print(final_output)

# âœ… ë©”ëª¨ë¦¬ ì •ë¦¬
if torch.cuda.is_available():
    torch.cuda.empty_cache()  # GPU ìºì‹œ ì •ë¦¬
elif hasattr(torch, "mps") and torch.backends.mps.is_available():
    torch.mps.empty_cache()  # Mac MPS ìºì‹œ ì •ë¦¬

gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
print("âœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ!")