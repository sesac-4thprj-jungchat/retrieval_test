# test
import os
import json
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.docstore.document import Document
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import re

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# Hugging Face API í† í° ì²´í¬
if not os.getenv("HUGGINGFACE_HUB_TOKEN"):
    raise ValueError("HUGGINGFACE_HUB_TOKENì´ .env íŒŒì¼ì— ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

# âœ… ë‚´ë¶€ LLM ëª¨ë¸ (ì˜ˆ: Llama2, KoGPT)
MODEL_NAME = "beomi/KoAlpaca-Polyglot-5.8B"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map="auto")

# `pad_token`ì„ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì • (ëª¨ë¸ ë¡œë“œ ì§í›„ í•œ ë²ˆë§Œ ì„¤ì •)
tokenizer.pad_token = tokenizer.eos_token

# None ë˜ëŠ” ë¹ˆ ë¬¸ìì—´ì„ "ì •ë³´ ì—†ìŒ"ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def clean_metadata(value):
    return value if value not in [None, ""] else "ì •ë³´ ì—†ìŒ"

# Step-Back ê¸°ëŠ¥ ì¶”ê°€ëœ í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜
def generate_step_back_text(obj, query=None):
    """
    Step-Back ìŠ¤íƒ€ì¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜:
    - ì…ë ¥ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë™ì ìœ¼ë¡œ FAQ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±.
    - ì‚¬ìš©ì ì§ˆë¬¸(query)ì„ ë°›ì•„ ì§ˆë¬¸ì„ ë” ì¶”ìƒì ì´ê³  ì¼ë°˜ì ì¸ í˜•íƒœë¡œ ë³€í™˜.
    """
    
    # 1. ì‚¬ìš©ì ì§ˆë¬¸ì„ ì¶”ìƒí™”í•˜ì—¬ Step-Back ì²˜ë¦¬
    step_back_question = (
        "ì´ ì„œë¹„ìŠ¤ì˜ ì „ë°˜ì ì¸ ì •ë³´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
        if query is None
        else f"'{query}'ì™€ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ìš”ì•½í•˜ì„¸ìš”."
    )
    
    # 2. FAQ ìŠ¤íƒ€ì¼ ì •ë³´ ìƒì„±
    faq_text = (
        f"Q: ì´ ì„œë¹„ìŠ¤ì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?\nA: {clean_metadata(obj.get('ì„œë¹„ìŠ¤ëª…'))}\n\n"
        f"Q: ì´ ì„œë¹„ìŠ¤ì˜ IDëŠ” ë¬´ì—‡ì¸ê°€ìš”?\nA: {clean_metadata(obj.get('ì„œë¹„ìŠ¤ID'))}\n\n"
        f"Q: ì´ ì„œë¹„ìŠ¤ëŠ” ì–´ë–¤ ê¸°ê´€ì—ì„œ ì œê³µí•˜ë‚˜ìš”?\nA: {clean_metadata(obj.get('ë¶€ì„œëª…'))}\n\n"
        f"Q: ì´ ì„œë¹„ìŠ¤ëŠ” ì–´ë–¤ ë¶„ì•¼ì— ì†í•˜ë‚˜ìš”?\nA: {clean_metadata(obj.get('ì„œë¹„ìŠ¤ë¶„ì•¼'))}\n\n"
        f"Q: ì´ ì„œë¹„ìŠ¤ì˜ ëª©ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\nA: {clean_metadata(obj.get('ì„œë¹„ìŠ¤ëª©ì ìš”ì•½'))}\n\n"
        f"Q: ì´ ì„œë¹„ìŠ¤ë¥¼ ë°›ì„ ìˆ˜ ìˆëŠ” ëŒ€ìƒì€ ëˆ„êµ¬ì¸ê°€ìš”?\nA: {clean_metadata(obj.get('ì§€ì›ëŒ€ìƒ'))}\n\n"
        f"Q: ì§€ì› ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?\nA: {clean_metadata(obj.get('ì§€ì›ë‚´ìš©'))}\n\n"
        f"Q: ì´ ì„œë¹„ìŠ¤ì˜ ì„ ì • ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?\nA: {clean_metadata(obj.get('ì„ ì •ê¸°ì¤€'))}\n\n"
        f"Q: ì´ ì„œë¹„ìŠ¤ì˜ ì§€ì› ìœ í˜•ì€ ë¬´ì—‡ì¸ê°€ìš”?\nA: {clean_metadata(obj.get('ì§€ì›ìœ í˜•'))}\n\n"
        f"Q: ì–¸ì œê¹Œì§€ ì‹ ì²­í•  ìˆ˜ ìˆë‚˜ìš”?\nA: {clean_metadata(obj.get('ì‹ ì²­ê¸°í•œ'))}\n\n"
        f"Q: ì‹ ì²­ ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?\nA: {clean_metadata(obj.get('ì‹ ì²­ë°©ë²•'))}\n\n"
        f"Q: ì–´ë””ì—ì„œ ì‹ ì²­í•  ìˆ˜ ìˆë‚˜ìš”?\nA: {clean_metadata(obj.get('ì ‘ìˆ˜ê¸°ê´€'))}\n\n"
    )
    
    # 3. ìì—°ì–´ ìŠ¤íƒ€ì¼ ë¬¸ì¥ ìƒì„± (ì¢…í•©)
    summary_text = (
        f"{clean_metadata(obj.get('ì„œë¹„ìŠ¤ëª…'))} ì„œë¹„ìŠ¤ (ì„œë¹„ìŠ¤ID: {clean_metadata(obj.get('ì„œë¹„ìŠ¤ID'))})ëŠ” "
        f"{clean_metadata(obj.get('ë¶€ì„œëª…'))}ì—ì„œ ìš´ì˜í•˜ëŠ” {clean_metadata(obj.get('ì„œë¹„ìŠ¤ë¶„ì•¼'))} ë¶„ì•¼ì˜ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. "
        f"ì´ ì„œë¹„ìŠ¤ëŠ” {clean_metadata(obj.get('ì„œë¹„ìŠ¤ëª©ì ìš”ì•½'))}ì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. "
        f"ì´ ì„œë¹„ìŠ¤ë¥¼ ë°›ì„ ìˆ˜ ìˆëŠ” ëŒ€ìƒì€ {clean_metadata(obj.get('ì§€ì›ëŒ€ìƒ'))}ì…ë‹ˆë‹¤. "
        f"ì£¼ìš” ì§€ì› ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: {clean_metadata(obj.get('ì§€ì›ë‚´ìš©'))}. "
        f"ì´ ì„œë¹„ìŠ¤ì˜ ì„ ì • ê¸°ì¤€ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: {clean_metadata(obj.get('ì„ ì •ê¸°ì¤€'))}. "
        f"ì§€ì› ìœ í˜•ì€ {clean_metadata(obj.get('ì§€ì›ìœ í˜•'))}ì…ë‹ˆë‹¤. "
        f"ì‹ ì²­ ê¸°í•œì€ {clean_metadata(obj.get('ì‹ ì²­ê¸°í•œ'))}ê¹Œì§€ì´ë©°, ì‹ ì²­ ë°©ë²•ì€ {clean_metadata(obj.get('ì‹ ì²­ë°©ë²•'))}ì…ë‹ˆë‹¤. "
        f"ê´€ë ¨ ë¬¸ì˜ ë° ì‹ ì²­ì€ {clean_metadata(obj.get('ì ‘ìˆ˜ê¸°ê´€'))}ì—ì„œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )
    
    # 4. Step-Back ì§ˆë¬¸ê³¼ ê²°í•©
    step_back_text = (
        f"### Step-Back Question\n"
        f"{step_back_question}\n\n"
        f"### FAQ ìŠ¤íƒ€ì¼ ì •ë³´\n"
        f"{faq_text}\n\n"
        f"### ì¢…í•© ìš”ì•½\n"
        f"{summary_text}"
    )
    
    return step_back_text

# JSON ë°ì´í„° ë¡œë“œ
json_file_path = r"/home/elicer/embedding2/FinalPJ/20250304.json"
try:
    with open(json_file_path, "r", encoding="utf-8") as f:
        json_data = json.load(f)
except FileNotFoundError:
    raise FileNotFoundError(f"JSON íŒŒì¼ '{json_file_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
except json.JSONDecodeError:
    raise ValueError(f"JSON íŒŒì¼ '{json_file_path}'ì˜ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")

if not isinstance(json_data, list):
    raise ValueError("JSON ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœê°€ ì•„ë‹™ë‹ˆë‹¤.")

# Document ê°ì²´ë¡œ ë³€í™˜
documents = []
for obj in json_data:
    text = generate_step_back_text(obj)
    metadata = {
        "ì„œë¹„ìŠ¤ID": clean_metadata(obj.get("ì„œë¹„ìŠ¤ID")),
        "ì„œë¹„ìŠ¤ëª…": clean_metadata(obj.get("ì„œë¹„ìŠ¤ëª…")),
        "ì„œë¹„ìŠ¤ëª©ì ìš”ì•½": clean_metadata(obj.get("ì„œë¹„ìŠ¤ëª©ì ìš”ì•½")),
        "ì‹ ì²­ê¸°í•œ": clean_metadata(obj.get("ì‹ ì²­ê¸°í•œ")),
        "ì§€ì›ë‚´ìš©": clean_metadata(obj.get("ì§€ì›ë‚´ìš©")),
        "ì„œë¹„ìŠ¤ë¶„ì•¼": clean_metadata(obj.get("ì„œë¹„ìŠ¤ë¶„ì•¼")),
        "ì„ ì •ê¸°ì¤€": clean_metadata(obj.get("ì„ ì •ê¸°ì¤€")),
        "ì‹ ì²­ë°©ë²•": clean_metadata(obj.get("ì‹ ì²­ë°©ë²•")),
        "ë¶€ì„œëª…": clean_metadata(obj.get("ë¶€ì„œëª…")),
        "ì ‘ìˆ˜ê¸°ê´€": clean_metadata(obj.get("ì ‘ìˆ˜ê¸°ê´€"))
    }
    documents.append(Document(page_content=text, metadata=metadata))

# ì„ë² ë”© ëª¨ë¸ ì„¤ì •
embedding_model = HuggingFaceEmbeddings(
    model_name="dragonkue/snowflake-arctic-embed-l-v2.0-ko",  # ê¸°ì¡´ê³¼ ë™ì¼í•œ ëª¨ë¸ ì‚¬ìš©
    model_kwargs={'device': 'cuda'},
    encode_kwargs={'normalize_embeddings': True}
)
# # FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë° ì €ì¥
persist_directory = r"/home/elicer/embedding2/FinalPJ"
# os.makedirs(persist_directory, exist_ok=True)

# try:
#     vectorstore = FAISS.from_documents(documents, embedding_model)
#     vectorstore.save_local(persist_directory)
#     print(f"ì´ {len(documents)}ê°œì˜ ë¬¸ì„œê°€ FAISS ë²¡í„°DBì— ì €ì¥ë¨.")
#     print(f"ì €ì¥ ê²½ë¡œ: {persist_directory}")
# except Exception as e:
#     print(f"FAISS ë²¡í„° DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# FAISS ë¡œë“œ ë° ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
try:
    vectorstore = FAISS.load_local(persist_directory, embedding_model, allow_dangerous_deserialization=True)
except Exception as e:
    raise Exception(f"FAISS ë²¡í„° DB ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# âœ… ì¼ë°˜ RAG ê²€ìƒ‰
def retrieve_documents(query, top_k=20):
    docs = vectorstore.similarity_search(query, k=top_k)
    return docs
#retriever = vectorstore.as_retriever(search_kwargs={"k": 20})  # âœ… retriever ì„¤ì •


def extract_rank_score(ranked_text, document):
    """
    GPTê°€ ìƒì„±í•œ ë­í‚¹ ê²°ê³¼ì—ì„œ í•´ë‹¹ ë¬¸ì„œì˜ ì ìˆ˜ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    """
    # ë¬¸ì„œ ë‚´ìš©ì˜ ì¼ë¶€ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²€ìƒ‰ (ì¼ì¹˜ìœ¨ ë†’ì´ê¸° ìœ„í•´ ì¼ë¶€ ë¬¸êµ¬ ì‚¬ìš©)
    doc_snippet = document.page_content[:30]  # ì²« 30ì ì •ë„ ì‚¬ìš© (ì¤‘ë³µ ë°©ì§€)

    # ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ ì ìˆ˜ ì¶”ì¶œ (ì˜ˆ: "ë¬¸ì„œ A - 95ì ")
    pattern = re.compile(rf"{re.escape(doc_snippet)}.*?(\d+)ì ", re.DOTALL)
    match = pattern.search(ranked_text)

    if match:
        return int(match.group(1))  # ì ìˆ˜ ë°˜í™˜
    return 0  # ì ìˆ˜ ì—†ìœ¼ë©´ 0ì 

def rank_documents_with_llm(query, documents):
    prompt = "ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ê°€ì¥ ê´€ë ¨ ìˆëŠ” ë¬¸ì„œë¥¼ ìˆœì„œëŒ€ë¡œ ë‚˜ì—´í•˜ê³  ê°ê°ì˜ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ 1ì ì—ì„œ 100ì  ì‚¬ì´ë¡œ ë§¤ê²¨ì£¼ì„¸ìš”.\n\n"
    for i, doc in enumerate(documents):
        prompt += f"{i+1}. {doc.page_content}\n"

    prompt += "\nê° ë¬¸ì„œì˜ ìˆœìœ„ì™€ ì ìˆ˜ë¥¼ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”: (ìˆœìœ„). (ë¬¸ì„œ ë‚´ìš© ì¼ë¶€) - (ì ìˆ˜)ì "

    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024).to("cuda")
    outputs = model.generate(inputs["input_ids"], max_new_tokens=200)

    ranked_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # ê²°ê³¼ì—ì„œ ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œë¥¼ ì •ë ¬ (ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì€ ë¬¸ì„œê°€ ìƒìœ„)
    ranked_docs = sorted(documents, key=lambda x: extract_rank_score(ranked_text, x), reverse=True)

    return ranked_docs[:15]  # ìƒìœ„ 15ê°œ ë¬¸ì„œ ë°˜í™˜


def step_back_rag(query, documents):
    context = "\n".join([doc.page_content for doc in documents])
    input_text = f"ì‚¬ìš©ì ì§ˆë¬¸: {query}\n\nì œê³µëœ ì •ë³´:\n{context}\n\nì œê³µëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”."

    # ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì¦ˆí•˜ë©´ì„œ, attention_maskì™€ pad_token_id ì„¤ì •
    inputs = tokenizer(
        input_text,
        return_tensors="pt",
        truncation=True,
        max_length=1024,
        padding=True  # ìë™ìœ¼ë¡œ íŒ¨ë”© ì²˜ë¦¬
    ).to("cuda")

    # attention_mask ëª…ì‹œì ìœ¼ë¡œ ì„¤ì • (íŒ¨ë”© í† í°ì„ 0ìœ¼ë¡œ ì„¤ì •)
    inputs['attention_mask'] = (inputs['input_ids'] != tokenizer.pad_token_id).long()

    # pad_token_idë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •
    outputs = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=300,
        pad_token_id=tokenizer.pad_token_id  # íŒ¨ë”© í† í° ëª…ì‹œì  ì„¤ì •
    )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰
query = "ì„œìš¸ì— 1ì¸ê°€êµ¬ ì €ì†Œë“ì¸µì— ë§ëŠ” ì„œë¹„ìŠ¤ë¥¼ ì°¾ì•„ì¤˜."

print("ğŸ” 1ë‹¨ê³„: ì¼ë°˜ RAG ê²€ìƒ‰")
retrieved_docs = retrieve_documents(query)

print("ğŸ“Š 2ë‹¨ê³„: RankGPT ëŒ€ì²´ (LLM ë­í‚¹)")
ranked_docs = rank_documents_with_llm(query, retrieved_docs)

print("ğŸ§  3ë‹¨ê³„: Step-Back RAG ì ìš©")
response = step_back_rag(query, ranked_docs)

print(response)